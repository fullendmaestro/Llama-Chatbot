{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fullendmaestro/Llama-Chatbot/blob/main/Llama_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-2 Enhanced Chatbot with Sentiment Analysis and Semantic Search\n",
        "\n",
        "## Overview\n",
        "\n",
        "This project implements a conversational AI chatbot using the **Llama-2 model** for text generation, integrated with **sentiment analysis** to improve user interactions. The chatbot is further enhanced by **semantic search** capabilities through **Pinecone** to retrieve contextually relevant information from multiple datasets, allowing it to provide more accurate and personalized responses.\n",
        "\n",
        "### Key Features:\n",
        "1. **Llama-2 Text Generation**:\n",
        "   - The chatbot uses the pre-trained Llama-2 model from Hugging Face for generating conversational responses. This model produces high-quality natural language outputs.\n",
        "   \n",
        "2. **Sentiment Analysis**:\n",
        "   - A sentiment analysis model from Hugging Face (`distilbert-base-uncased-finetuned-sst-2-english`) detects whether user input is positive, negative, or neutral. This enables the chatbot to adjust its tone accordingly, enhancing the interaction quality.\n",
        "\n",
        "3. **Semantic Search using Pinecone**:\n",
        "   - Pinecone's vector database is employed to perform semantic searches across three datasets:\n",
        "     - **QA Dataset**: Frequently asked questions and answers.\n",
        "     - **Product Dataset**: Product descriptions and metadata.\n",
        "     - **Troubleshooting Dataset**: Steps and solutions to resolve common issues.\n",
        "   - The chatbot uses **sentence-transformers** to generate vector embeddings of user queries and document texts, enabling quick and accurate searches based on semantic similarity.\n",
        "\n",
        "4. **Contextual Conversations**:\n",
        "   - The chatbot combines historical interactions with search results to generate more context-aware responses, making the conversation feel natural and connected to previous queries.\n",
        "\n",
        "### Datasets Used:\n",
        "- **`qa_dataset.csv`**: Contains Q&A pairs, helping the chatbot answer common user questions.\n",
        "- **`products.csv`**: A dataset of product descriptions used to retrieve product-related information.\n",
        "- **`troubleshooting.csv`**: Contains troubleshooting steps for resolving technical issues.\n",
        "\n",
        "### Integration:\n",
        "- **Hugging Face Transformers**: For text generation (Llama-2) and sentiment analysis models.\n",
        "- **Pinecone**: For fast, scalable vector-based searches using sentence embeddings.\n",
        "- **Gradio**: A web-based user interface that allows users to interact with the chatbot in real-time.\n",
        "\n",
        "### Architecture:\n",
        "1. **User Input**: The user asks a question or provides feedback.\n",
        "2. **Sentiment Analysis**: The chatbot analyzes the emotional tone of the input (positive, neutral, or negative).\n",
        "3. **Semantic Search**: The system queries the Pinecone index for relevant documents across multiple datasets.\n",
        "4. **Contextual Response Generation**: Llama-2 generates a response based on the search results, chat history, and detected sentiment.\n",
        "5. **User Response**: The chatbot tailors its reply to reflect the detected sentiment and retrieved information.\n",
        "\n",
        "---\n",
        "\n",
        "This project showcases a fully interactive and intelligent chatbot, capable of handling a variety of queries and responding dynamically based on the user's sentiment and contextual search results.\n",
        "\n"
      ],
      "metadata": {
        "id": "0QdDqJVFSgLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qldh4YOFPJni"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA4iS6Qao382",
        "outputId": "2ff2ee2d-c5a3-4de0-de40-754834803e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6lg43jn6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6lg43jn6\n",
            "  Resolved https://github.com/huggingface/transformers to commit e1b150862e66e16acf951edfa13206ffcd1032be\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.0.dev0)\n",
            "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=9920673 sha256=591742588da786627741a84c105b3589b3057c7a3123ff537f585197934c0668\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q8b0krgr/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.20.0 transformers-4.46.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWGkVIXHJDUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1e63da-87a2-41ea-fcc8-00951364e85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.1/378.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install openai pinecone-client gradio sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0qYFO8d3SEm"
      },
      "outputs": [],
      "source": [
        "# API Keys from colab secret for Hugging Face, OpenAI, and Pinecone\n",
        "from google.colab import userdata\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "huggingface_api_key = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "pinecone_region = userdata.get('PINECONE_ENV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmQWrMVIpW9G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwyGNz53qR0B",
        "outputId": "0bccee1e-3b82-4c2b-8502-384b497a5437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Hugging Face Authentication\n",
        "login(token=huggingface_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvcWebwwql0y"
      },
      "outputs": [],
      "source": [
        "# Define the paths for the CSV files\n",
        "products_csv = 'products.csv'\n",
        "qa_csv = 'qa_dataset.csv'\n",
        "troubleshooting_csv = 'troubleshooting.csv'\n",
        "\n",
        "\n",
        "\n",
        "# Load the existing CSV file into a DataFrame\n",
        "qa_df = pd.read_csv(qa_csv)\n",
        "products_df = pd.read_csv(products_csv)\n",
        "troubleshooting_df = pd.read_csv(troubleshooting_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWPB9c9Hqv14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f74aa81-48f1-4259-c15e-66868725d37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA Dataset:\n",
            "                                            Question  \\\n",
            "0  My SmartHome Hub won't connect to Wi-Fi. What ...   \n",
            "1  The temperature readings on my Smart Thermosta...   \n",
            "2  My Smart Lights won't turn on or off using the...   \n",
            "3  The Smart Lock isn't responding to app command...   \n",
            "4  My Smart Security Camera isn't showing a live ...   \n",
            "\n",
            "                                              Answer  \n",
            "0  I understand you're having trouble connecting ...  \n",
            "1  I'm sorry to hear your Smart Thermostat is sho...  \n",
            "2  I see you're having issues controlling your Sm...  \n",
            "3  I understand your Smart Lock isn't responding ...  \n",
            "4  I'm sorry to hear you're not getting a live fe...  \n",
            "\n",
            "Products Dataset:\n",
            "                           Title  \\\n",
            "0              SmartHome Hub Pro   \n",
            "1      EcoTherm Smart Thermostat   \n",
            "2      LuminaGlow Smart Bulb Set   \n",
            "3         SecureGuard Smart Lock   \n",
            "4  ClearView Pro Security Camera   \n",
            "\n",
            "                                         Description  \n",
            "0  Control your entire smart home ecosystem with ...  \n",
            "1  Save energy and money with the EcoTherm Smart ...  \n",
            "2  Transform your home lighting with the LuminaGl...  \n",
            "3  Enhance your home security with the SecureGuar...  \n",
            "4  Keep an eye on your property with the ClearVie...  \n",
            "\n",
            "Troubleshooting Dataset:\n",
            "                  Device                           Issue  \\\n",
            "0          SmartHome Hub          Won't connect to Wi-Fi   \n",
            "1       Smart Thermostat  Incorrect temperature readings   \n",
            "2           Smart Lights  Not responding to app commands   \n",
            "3             Smart Lock    Battery draining too quickly   \n",
            "4  Smart Security Camera              Poor video quality   \n",
            "\n",
            "                                               Steps  \n",
            "0  Ensure your Wi-Fi network is operational\\nChec...  \n",
            "1  Check if the thermostat is placed away from di...  \n",
            "2  Verify that your smartphone and smart lights a...  \n",
            "3  Check if the lock is properly installed with n...  \n",
            "4  Check your internet connection speed\\nEnsure t...  \n"
          ]
        }
      ],
      "source": [
        "# Verify the CSV content\n",
        "print(\"QA Dataset:\")\n",
        "print(qa_df.head())\n",
        "\n",
        "print(\"\\nProducts Dataset:\")\n",
        "print(products_df.head())\n",
        "\n",
        "print(\"\\nTroubleshooting Dataset:\")\n",
        "print(troubleshooting_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3PTvt2-hQ6j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoIRUs1vG4q9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "e96a6c4017fe46ffbaed658480a92056",
            "93191d9728cc4ccaa1766522205e4eee",
            "9aa94b2493cb4fe0a420f1c469002e2a",
            "1c6a6c9c05764d73bfdb1ffedcffd4e9",
            "bec6b13603ce4ee586c350f2471fe5fe",
            "b875b5d9aab74a32a6d114230cdea310",
            "52830b63c4114d1ca5a7066ce680515a",
            "21b2279d3ce542cf8f4da2682556b09f",
            "6590c1cadf4345ec92f8bf3c766d47ab",
            "55a48f22fc6e4339aaf6dfb4ecc27108",
            "2be6b799064044ac999fb169a24d6055",
            "284344e356f242769274b866c96ca49a",
            "6e4b13d80a824a228576674f39b23316",
            "33e40a6cc82249f8915d9bc919a60923",
            "acb4543502fd40b2a16bf7e4b1916d46",
            "04f3dc5010b74d2998fd5c9e1ab1ac31",
            "bd93f92034664d2b9181944e2e0f2653",
            "d0b3b15a3cb64a319c27601ac6fde3a2",
            "70a5bd622c09489d8bc701d57e174f9d",
            "797971a28ee64a54a1bc9cfc84494982",
            "2618aef601db4f63811e237cda612f4e",
            "e0714465150f42e38e90047237bfaf3c",
            "e24a9e29a70d452c917d1140ce10f836",
            "decfc30ea04346c7aea14f4504309791",
            "e22e26f1fd654aeca22954a1f3a3f2bf",
            "c1cb33515216491d8decc2c6b8136a56",
            "f78640b47e7644d9a523614514dc86ac",
            "1224d775488343939ceebc3d4c20f9d6",
            "e9ca514aa8f8418c9b78558ebb2669b3",
            "948cd3e54972411abb5ce15aeb1b9217",
            "9b3cd1edeaca4ce781dd5053effb4948",
            "26c2d8329d9c490aa4a729e4b7c9d67f",
            "cfa4511dae0d4d188c28460214b0d5b8",
            "8e9cf2c233ea487c811ac4cfcfebd830",
            "984b246d5e534d1b817442eeb305b773",
            "e0a242a59f814e7f8b86d18142813a14",
            "350c9490bad64559a77e4ce699f04793",
            "b7b5029f68cf4bec8a12ca1bc6828fb5",
            "217e4d95b310451b9fad62b2cf574a14",
            "f5cc271f481748239ce18b9deaba41f5",
            "c24f1fcc91c3431b81b10f0f1a120676",
            "4e5694366c2f4ac4b6e62075397cbd15",
            "052e59f1dcac41cfb4f8be2a4627815a",
            "21e271833cd04fcfa71203ac2a721c08",
            "608c6bb2d93a4b128d6883e0b910e736",
            "6a70af6925b140c7a6d9868137b1629c",
            "aaca445edfea46babaafde7bc87e64f8",
            "b8d538a38ee84dd9b821d6d00ab4cc6d",
            "38c96bf9db224acaa249647e68cb18d9",
            "817ddb1e6c0a48d08525ab6e33a623aa",
            "534554a0a6164b4bb13894e8fdbc8c12",
            "70fb515a9fbd4a4bb3ef9920a7e14809",
            "ecf4058bc0f04f059029b0a2403ca7db",
            "24dbb6806f89424e863f5aa5bbec2692",
            "1ff2f89b20a942489cdcfc59bd452141",
            "1a4a954c1034473e816cdd475e041c2f",
            "de6ed96b99bc46fbbcfa9b400d5b8523",
            "a94813da48b6484ebf3b0b395adca80c",
            "ef1bebb6c3584f579f32cd9f67015da5",
            "ee4afd76e8974c42ae24fe7ddbc15e0e",
            "b1dbb8551f224be6a1ffe3dc66dddbbb",
            "32dcbe8ebb0844f6b15798bb49ec6717",
            "e90ff8e440ec466dadf9e47a3a671432",
            "88bbee4c4d9d45d886e429ed0df33ad4",
            "3cba227dfd6d402298cd8eda0c44b7a0",
            "5a90b94d2fa84da698da7748da0612e9",
            "de3ed29e98ef4b039fa5e304f377bd01",
            "9d995755804044619562a52bb3c2d23e",
            "8399a4d8c1314f9ea3e48bd2fdb07eea",
            "eaa5ca555262425da9fcaad32e714cfb",
            "7d3f4354c20c43f5b2ab2027b542231d",
            "71fbde4403bb46c0bc33460622af8dc3",
            "9c39ad6fb8a74300949726eee1a0d2c7",
            "61859f75d9dc4f9db34fcb3870feb94b",
            "353d3dbba8c949ae9d062a5bb88f90d4",
            "8211be706d9641bfb6b2028440375952",
            "fca5e3e3ba1142d7aae421590bb6923d",
            "94a9880d87b64b849675e23b6d2add81",
            "eebb5cb1b32f4af2b487bef010849ac3",
            "d3a05d26d7df4814a5feeeba9b45f48b",
            "e891b6da910c4a758b6ca3cdb995294b",
            "70558d67c1744fa7b10b37c6250087ac",
            "4f05697435bb44f28aa85ac1f79c929d",
            "9ddfadff16a145379552cbc56ce48976",
            "5f3dce4cbce0492d94d35c197624cb62",
            "0f9708f0fc97493a905794c45963393a",
            "53dd42ac89b547f8910abb6b37679867",
            "e85b050febc940d28a964f6f465e5c8a",
            "11ec5cc0829d46c89e7ad7632fd151e3",
            "19fea4c72be2406c8445d3ad9cb6a6d5",
            "d57c16ca2e9344f9bc9de41efd34e939",
            "010227c586cc4be281200058ae6df41a",
            "64a60e7c83a24d95b1a8ce5cdbd2e19a",
            "dfd436b4d1654b1bbb226ce38526477f",
            "bcb44c780a8b4bf298d22b7926438e5a",
            "da88f83b60e34abfad4882618b71bdfd",
            "1720c27d74c84bf08958c11c97677530",
            "307ae29e990a4e9e8239d4f73fe23995",
            "7e963e1a29624a019c1e6e81667c6525",
            "0e43e87ad0504b78b86bc9920361cbee",
            "070ddd7fee1d42fe86dd12a402a6f2aa",
            "fb130c29133c40578735df90a1fc6280",
            "a5113da89e084103865715b127444ca6",
            "dde96cf50fa24376ad9fbab713e2da78",
            "f217e917ec1b4fae90698e599746f9e2",
            "f5a1a52e2ee3471fad7fbaba843d1a0c",
            "85437dd7b00e42f09869a168f073bf96",
            "4a40547de4f1442aac67d91fcddec885",
            "d2ee182daa2b4cdfa1572d7da6b006f7",
            "f6362ae58c5447c3b318baf8dbd07071",
            "ba13760a180b4952902c35863ce779f9",
            "01468f38e04a4a23aec39edf5c5e6b01",
            "b6ef98e9ec364f9a9db83139aa28b935",
            "6960cd938ef44e4db3879b38659f95d7",
            "acfb0b72b0bd41838db87a68bf0f52fa",
            "d218ed5f92504617b5747381dfc8e012",
            "dd82cb612c3b4757b58630959b2495d0",
            "4d6b68207c324e1f9edd9ddcce886b55",
            "228450f19fb44d10a5303c33ef72784a",
            "f8a13304aa85414fa124e187660ea18b",
            "0776ce54343a4ae1b9140b1436fc920c"
          ]
        },
        "outputId": "e308a745-abdf-4e2a-c2ff-a5112fc97582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e96a6c4017fe46ffbaed658480a92056"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "284344e356f242769274b866c96ca49a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e24a9e29a70d452c917d1140ce10f836"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e9cf2c233ea487c811ac4cfcfebd830"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "608c6bb2d93a4b128d6883e0b910e736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a4a954c1034473e816cdd475e041c2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de3ed29e98ef4b039fa5e304f377bd01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94a9880d87b64b849675e23b6d2add81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11ec5cc0829d46c89e7ad7632fd151e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e43e87ad0504b78b86bc9920361cbee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba13760a180b4952902c35863ce779f9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import openai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# Load the sentence transformer model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Pinecone index name\n",
        "index_name = \"llama-quest-index\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNxr5CYJFuLV"
      },
      "outputs": [],
      "source": [
        "# # Check if the index exists, if not, create it\n",
        "# if index_name not in pc.list_indexes():\n",
        "#     pc.create_index(\n",
        "#         name=index_name,\n",
        "#         dimension=768,  # Embedding dimension for 'sentence-transformers/multi-qa-mpnet-base-cos-v1'\n",
        "#         metric=\"cosine\",\n",
        "#         spec=ServerlessSpec(\n",
        "#             cloud=\"aws\",\n",
        "#             region=pinecone_region\n",
        "#         )\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BTfveCUGHeA"
      },
      "outputs": [],
      "source": [
        "# Connect to the Pinecone index\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHkSCn0zzBYE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSbBtsySHS3g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFJONp7QHZER"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8mwwbHcq_oK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "1cc0791a20c34239b81b09cd9429e6d3",
            "f62750f6ce2d4ec89856d03859c2018a",
            "e7616cda9cdf4f7d9489798dd07301f0",
            "59650b4c8bd14fc2ab57281530371a33",
            "b76703f52ce4420395e9b567dc296ad7",
            "937a7d6819d04d8493b0d530deef2b78",
            "b25e2e7f641d44af82a1feaf31d86e67",
            "296b0977cc5b460ca6acd4d077fa27b3",
            "96e22d2e3fcb4178a1cca7344c9a305b",
            "f11b3fc8946d4fb78ee48c321cfd1a0c",
            "84f195f849d446b99f205401cd5a7e93",
            "a725acb2d9af4700a393b64274838d25",
            "dca217d4224c4564bdb950420c2ddb0f",
            "3d716866c1484f87a1463ada1a26527a",
            "91c647a894244403b6f1736574b2b904",
            "4493aec62fb34c779101c8506a64c22a",
            "34cf9194eda64f319c30925e675e5c5a",
            "c29e656bcf5247a4921f42019ae98ecb",
            "6b322237560b4b67ac0fa64640df870e",
            "7b063de84d4d4968b866198e65b81914",
            "82bda6258ed54f7395aff4efe7bdf6e8",
            "b2e8dacc4345446ea44bd3fc3f653965",
            "2bcae692776444ef9cdac865b49df5e6",
            "c6587a23729d495cab319ac191780a35",
            "d62c8381ceb14adaab479ade44bd75ea",
            "b0cd40f8e9a7404d8ceb8d9a5ab9b3e8",
            "b892a2396cc94d64ba0de73728889172",
            "6db999d796094e8181f474599d3b22c6",
            "33ca0708e7324a609ba1b9e7438749a1",
            "9fa641691160423081c20cda86c1bd1c",
            "162188a317ad44bea38ad72fd71ab73a",
            "466c9505cc4c427a9bf1e4f3e10e6b97",
            "7fa81fb8cfdb45a1bf920a85e371f389",
            "e78e4eba866240dca5e3b42d9c1ba624",
            "32bcffc23317492dbbb6b62d41bce99c",
            "b7d1c93162a944e99693de8875d57040",
            "bb0efeea46ae4bff855ad0ba88209a80",
            "7804f2077451482382bb019b1ad0a5fe",
            "ee8faedb917e4060ab99cf3a8fc5f23f",
            "ab3ceecf37e94cce84c7c9d0c38d9db4",
            "0f132d9316744fc3b8c9f7de1aaba467",
            "5007461e3bd54497b152096651e429d5",
            "0760574728cc4cab976fcff27c40434a",
            "d819c285d15f4e45b65ab069cfab6ac5",
            "d596d8140b59489e9b5c47c11b17c75a",
            "ff38d1e3585b48d4bc6228c616b6a86f",
            "af81ab85d44a4c258ddad59468ab7b98",
            "2e2f3b93ae6349ea8fa63e8cedebcb29",
            "e15afdabacb44a86bdeb08e7111e4a7f",
            "f6d7be3855f74cefbcc3c2e3b5bda037",
            "f562a920e7854f329c6c7e1100ad2111",
            "ae8baa9c09e342329cce11090199adac",
            "8d74dff1268d4328b5e24d158d688f49",
            "a782573c48344769abe95e0283028d49",
            "f6c2e41fa028494ab8b6bfce47a13948",
            "526b5da3bd4b4713966a67769ab6855f",
            "5ee6c034b1124732b0b4b516c83271a4",
            "39ccd79aef464aae84fe5ddac1f73ea7",
            "e3b23c06e01a48cb8965a0bdd490997d",
            "dc65e03ebe224322ad6317a4d6a68df8",
            "dc7015cf663a4ca3b4a1830febc2627c",
            "ac3a5f0252424a99a1362e719935ac68",
            "a6641bbdfd76424785a59987b29be6cc",
            "8c994d38c38b4a638308e4b6801996c0",
            "47f3232ef8f54b7f94a4d16040397d3f",
            "5fd8d076ba2145edb925a7ae25b52f01",
            "bd9e18fe21ec4d86bf91cf827b29cf45",
            "aa63485db4284acd8262abb2707d3bcb",
            "2ad83690af304f61b9db112b98c2b160",
            "df8e4dda7fa449808d8d4a2d6914f58e",
            "286c90498af74cc398ee3682b2676a84",
            "3e51f202736c422bb66d5225123522fb",
            "1496fa213715480b93777416f90dd56b",
            "1983969cdc8549a6a88f990a291a1ee1",
            "efa7ebc9c4c64f6f87667acb6a7f3586",
            "80fe18ce08064662a3545c2de0ed9916",
            "97e77d5a99664628b651223d10f7a2a0",
            "756538674ae74ff895247e1f9f334a5d",
            "749e4c6f004d4b03ad591096282d297e",
            "aef7243500814eafbcd8fe5bba6bf7ee",
            "3273ba6dc4e0402686ae5fc529de32a3",
            "de8da9d22d0d4b2e955b453fff3c9432",
            "493a9d3cac584d67aee60cbd6e34ad42",
            "b8108db26fd84bb8825b51dfbe5087f1",
            "1111d416321249839f6c4b9501412c85",
            "6a0ec256da2a48ac92eebd2a6e14ab8e",
            "78ffb222dc704eb1a379470da9ebc1f8",
            "14d5e7a161554f52896576d9ff0d98c4",
            "1f6badc2c1ff43759541453b93805837",
            "ea7ed34643cc4066881965f2c736e7ab",
            "41113d62386747b7a4a40e5c1226210f",
            "2812d090b7344ee384ec558eca74f36e",
            "6cb8ff2e98944ee89181e13978f6fb54",
            "4553e60a105f43adba7841791c090a05",
            "9b502cf4efbc4fb2b46afe6175d5cf75",
            "e8938c9efdde4150af3df14dbcf1169e",
            "c7c91a024e144093a2323bb691ec2e26",
            "0012d6f81e044851865d1a95618b647f",
            "7adc4e583fb64585bf7e07dee08ce159",
            "a1a9ee78330f4fdb8a47c13df4c057aa",
            "99c378c3a72c4b2486f6d80bf0cf49c7",
            "e13cf5928b254f4fb2ce0bf0e0a1cb1d",
            "b4fbc887166e4d6f9a022e745ad8ba9d",
            "54026a4e88eb4ce6b5d626c0f0d5ce11",
            "92421f9a913f45cf83926e147730e08d",
            "eb7f60407fb0440d9393adc0d279aec6",
            "9c6fd4fc64184bdcae59957921d30765",
            "cab7060637f84a1d846185b89acd3a65",
            "58af01d45fa4496dbc85621951662c7b",
            "b1a46e70bc7444d1aeaff3083f6fca3b",
            "ddbb6349c538454e8e2ed3432e2d72d0",
            "0d6d244450bd4cfbb5134b6233410245",
            "a05ba7a6989a48e881ee11ae62c42032",
            "46a49e16510b482eb56ae70b3da510e9",
            "420c41a28ee549a789be758ed11c4927",
            "eed2bef445a044e2ae215d16ff54b4b2",
            "ffcbed37575942f296e5a27997cbd382",
            "7b6146cc57c8444e9f48f326c76dc9c0",
            "a0f401300d56494283bb42b4cd8c78d8",
            "753c913aa4ae4cdc829cbf5c504ceeed",
            "8e620f6093b7497b8da02dc483f07751",
            "817bbc61777f47928dd938a778497616",
            "9ff0215d62f0462a9c384a91f5b8afb0",
            "5a03823418ef4260b45bd6014fef7d6b",
            "307534e19c4c447a84782bdaa34d6a01",
            "860e513e4e264799985555eb63a65dad",
            "bf57d560ff6d420eb215b96fb5a9e499",
            "34438569504146238bbe92147486a806",
            "b9ea0935a4c547e7af545d806b5ec8ef",
            "1ca3c8e212174037a25ba252bc081291",
            "b7ae3b6fa1624a68a0a05f55f3bab4a5",
            "4fe239126ce64e74a5d75de79a3f8d45"
          ]
        },
        "outputId": "f7ccf329-ab9f-43cc-96e4-fdde87911315"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cc0791a20c34239b81b09cd9429e6d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a725acb2d9af4700a393b64274838d25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bcae692776444ef9cdac865b49df5e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e78e4eba866240dca5e3b42d9c1ba624"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d596d8140b59489e9b5c47c11b17c75a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "526b5da3bd4b4713966a67769ab6855f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd9e18fe21ec4d86bf91cf827b29cf45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "756538674ae74ff895247e1f9f334a5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f6badc2c1ff43759541453b93805837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1a9ee78330f4fdb8a47c13df4c057aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddbb6349c538454e8e2ed3432e2d72d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "817bbc61777f47928dd938a778497616"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize the Llama 2 model and tokenizer\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n",
        "\n",
        "# Initialize the pipeline using Hugging Face pipeline\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1024,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load a sentiment analysis model from Hugging Face\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "\n",
        "# New function to analyze the sentiment of the user's input\n",
        "def analyze_sentiment(user_input):\n",
        "    result = sentiment_analyzer(user_input)[0]\n",
        "    sentiment = result['label']\n",
        "    return sentiment.lower()  # Convert sentiment to lowercase to match categories"
      ],
      "metadata": {
        "id": "OjoHbrWjHrHC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "805632dfd147471a96d57922ab95b3eb",
            "bfb824831ec24489a2c9c13b85c107b5",
            "12d7e9d5f4724367940ecb33cd763dfb",
            "47d9a016e83e4f06831ad528d166a24a",
            "71c7fedf20c9429497949ddf17fa19e7",
            "ff49611e96334d68b5f56597a7ce6f57",
            "3b36f598df524d05913a936da3531666",
            "65c2bfaa3ab04ddabcc1f674f1366727",
            "d624401a0ef844839560b6030eb091e4",
            "fd12a8eba6a24a9290521c92c314caa2",
            "44a95678d11b419cb216eb8c9a9b8c01",
            "b28514ffca754d72afceb90c56fcb1f6",
            "b7cf0db3cd764ea5b15dd6085f36cf28",
            "702a1691a8e24aa08daf4c6c03216b1d",
            "f6dc2d41f31648d190c56cddc7b2989b",
            "4339033fa5e5479b92da467f2da43e60",
            "422081dfc39f41458e1ee7ec369e0208",
            "668d61bf2a7a47f3bed4a85b10b0659f",
            "472f2797ada54c4a91223e4aaecf996b",
            "3bb5050d347c42c6a938c8b400f57f4a",
            "6277950da1a14f1192277f6262fb7233",
            "d91bda7256e548a9a6919dda7fe35d11",
            "a198c0b986d34501a344c20244615718",
            "1963a7c09c434c6d91ad9040d0194c02",
            "c768485bb1244167be82125e64fd6498",
            "839160adfff24cdd89aa1e8098c95fcd",
            "bcb850cc4d284113bed54060c9c9a6d3",
            "f3050a9cea634ac3a012296329671f26",
            "515c28c8b6594dfd826b07154e9e8af8",
            "2d2b631310224e008a7ad9ecf1109372",
            "597c798ff37e405ca3ba65956002edae",
            "08eeacf69e654f36912c8ea8e8d88c32",
            "99f62de51d7d41d791cd4b75238b8be6",
            "084d8a1a701d41a7b99c297bd50cff78",
            "fe449e7fe98e4e0c80fae0a706eac05f",
            "520ed08762a04ebe82898e26d052f65a",
            "ae297c2c1e554eec8442476857989a4a",
            "830017170ccd4df78251d84233c5787e",
            "f3bc98a68ca74b12a1a9316f9a838785",
            "599f7788044d47af9ca06ccdf37006ac",
            "7b75890ccf5f4fdea5f12d5b0fc197f1",
            "9b5f7170b14e458fa11d6834bdaf08a7",
            "e7dac64ffd194016b4692bfa0057e2d8",
            "7a3a7dae95324dcd8012660fc4ef38d1"
          ]
        },
        "outputId": "a8004375-b39f-4bfb-890d-608e6986ed21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "805632dfd147471a96d57922ab95b3eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b28514ffca754d72afceb90c56fcb1f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a198c0b986d34501a344c20244615718"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "084d8a1a701d41a7b99c297bd50cff78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_lDnc-yvxUf"
      },
      "outputs": [],
      "source": [
        "# Debugging function to print the current state of history\n",
        "def print_history_debug(history):\n",
        "    print(\"\\n----- Current History -----\")\n",
        "    for idx, (user_input, bot_response) in enumerate(history):\n",
        "        print(f\"Turn {idx+1}:\")\n",
        "        print(f\"User: {user_input}\")\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "    print(\"----------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_iL0VkfGKjg"
      },
      "outputs": [],
      "source": [
        "# Embed and store documents in Pinecone\n",
        "def embed_and_store_documents(df, text_column, namespace, metadata_columns=None):\n",
        "    if metadata_columns is None:\n",
        "        metadata_columns = []  # Ensure it's an empty list if not provided\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Handle text_column being a list or string\n",
        "        text = row[text_column] if isinstance(text_column, str) else ' '.join([str(row[col]) for col in text_column])\n",
        "\n",
        "        # Embed the text using the sentence-transformers model\n",
        "        embedding = embedding_model.encode(text).tolist()  # Convert to list format for Pinecone\n",
        "\n",
        "        # Extract metadata\n",
        "        metadata = {col: row[col] for col in metadata_columns}  # Handle metadata columns\n",
        "\n",
        "        # Insert into Pinecone with unique ID (index)\n",
        "        index.upsert([(str(idx), embedding, metadata)], namespace=namespace)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0TtSqO9bKoh"
      },
      "outputs": [],
      "source": [
        "# Function to load datasets and index them correctly\n",
        "def index_datasets():\n",
        "    # Load datasets\n",
        "    qa_df = pd.read_csv('qa_dataset.csv')\n",
        "    products_df = pd.read_csv('products.csv')\n",
        "    troubleshooting_df = pd.read_csv('troubleshooting.csv')\n",
        "\n",
        "    # Index QA dataset: Use \"Answer\" as document, \"Question\" as metadata\n",
        "    embed_and_store_documents(qa_df, 'Answer', 'qa', metadata_columns=['Question'])\n",
        "\n",
        "    # Index Products dataset: Use \"Description\" as document, \"Title\" as metadata\n",
        "    embed_and_store_documents(products_df, 'Description', 'products', metadata_columns=['Title'])\n",
        "\n",
        "    # Index Troubleshooting dataset: Concatenate \"Issue\" and \"Steps\" for document, \"Device\" as metadata\n",
        "    embed_and_store_documents(troubleshooting_df, ['Issue', 'Steps'], 'troubleshooting', metadata_columns=['Device'])\n",
        "\n",
        "# Run the indexing process\n",
        "# index_datasets()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJaSPHskHvRb"
      },
      "outputs": [],
      "source": [
        "# Semantic search function using Pinecone\n",
        "def semantic_search(query, namespace):\n",
        "    # Use sentence-transformers to encode the query\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "\n",
        "    # Query Pinecone\n",
        "    results = index.query(\n",
        "        vector=query_embedding,  # The query embedding\n",
        "        top_k=3,  # Top 3 results\n",
        "        include_metadata=True,  # Include metadata in the results\n",
        "        namespace=namespace  # Search in the specified namespace\n",
        "    )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4EBVJzUbp6c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs55mw_IKnY7"
      },
      "outputs": [],
      "source": [
        "# Modify the generate_with_context function to take sentiment into account\n",
        "def generate_with_context(question, search_results, history, sentiment):\n",
        "    # Prepare the context from search results and history\n",
        "    if search_results:\n",
        "        best_match = search_results[0]['metadata']  # Assuming metadata holds relevant information\n",
        "        context = f\"Context: {best_match}\\n\\n\"\n",
        "    else:\n",
        "        context = \"\"\n",
        "\n",
        "    # Format the history into the prompt\n",
        "    conversation_history = \"\"\n",
        "    for user_input, bot_response in history:\n",
        "        conversation_history += f\"User: {user_input}\\nAI: {bot_response}\\n\"\n",
        "\n",
        "    # Combine context, history, and new question\n",
        "    full_prompt = f\"{context}{conversation_history}User: {question}\\nAI:\"\n",
        "\n",
        "    # Adjust the response based on the sentiment\n",
        "    if sentiment == 'negative':\n",
        "        full_prompt = f\"Be empathetic in your responses.\\n{full_prompt}\"\n",
        "    elif sentiment == 'positive':\n",
        "        full_prompt = f\"Be friendly and upbeat in your responses.\\n{full_prompt}\"\n",
        "    else:\n",
        "        full_prompt = f\"Be neutral and formal in your responses.\\n{full_prompt}\"\n",
        "\n",
        "    # Generate the response using Llama-2\n",
        "    response = llama_pipeline(\n",
        "        full_prompt,\n",
        "        max_new_tokens=150,  # Specify how many tokens the model should generate in the response\n",
        "        do_sample=True\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    # Remove the original prompt from the generated response\n",
        "    response_text = response[len(full_prompt):].strip()  # Only return the model's response text\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsrBNaK7uWH_"
      },
      "outputs": [],
      "source": [
        "# Main function to handle a user's question and provide an answer\n",
        "def answer_question(question, history):\n",
        "    # Perform semantic search across multiple namespaces (QA, products, troubleshooting)\n",
        "    search_results = []\n",
        "    try:\n",
        "        search_results.extend(semantic_search(question, 'qa').matches)\n",
        "        search_results.extend(semantic_search(question, 'products').matches)\n",
        "        search_results.extend(semantic_search(question, 'troubleshooting').matches)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during semantic search: {e}\")\n",
        "        return \"Error during semantic search.\"\n",
        "\n",
        "    # Analyze sentiment of the user's question\n",
        "    sentiment = analyze_sentiment(question)\n",
        "\n",
        "    # Generate answer using the search results as context, conversation history, and sentiment\n",
        "    try:\n",
        "        response = generate_with_context(question, search_results, history, sentiment)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during response generation: {e}\")\n",
        "        return f\"Error during response generation: {e}\"\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzRM45pNu7o8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz_QGPnavIJA"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Gradio chat interface\n",
        "def gradio_chat_interface(question, history):\n",
        "\n",
        "    response = answer_question(question, history)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRDJpTtfymFK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrlboOxvvetW"
      },
      "outputs": [],
      "source": [
        "# Create a Gradio Interface\n",
        "interface = gr.ChatInterface(fn=gradio_chat_interface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD2Hsg6hvjYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "63409cf8-2fa3-439b-c41d-4cc73bf33b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fdaa8d6426ae0a2f1e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fdaa8d6426ae0a2f1e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Launch the Gradio Interface\n",
        "interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4dTn2pZXhGy"
      },
      "outputs": [],
      "source": [
        "# res = answer_question(\"What is the price of the iPhone 12?\", [])\n",
        "# res"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNqSktK0vBWfamSoyHDbWuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}